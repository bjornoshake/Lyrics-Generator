{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LyricsGenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/xMMN0vvAnObLNSKJKo0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjornoshake/Lyrics-Generator-/blob/gh-pages/LyricsGenerator-GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr1qZ5dia1Wa",
        "outputId": "4f69dcea-9ce7-4a6d-9aa9-26db5261b450"
      },
      "source": [
        "! pip install gpt-2-simple\n",
        "! pip install tensorflow==1.13.2\n",
        "\n",
        "\n",
        "\n",
        "# First start by having the module, else it would be difficult to use it"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/c9/44fe420225244ab9e3f2938a1d11651681078cf72f7444c66d0ea49f1320/gpt_2_simple-0.7.2.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.19.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/7d/55784e894ee0cde2474fb977ffd1651e74e840a9f92e1d847f7e3115d5ec/toposort-1.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.2-cp37-none-any.whl size=23621 sha256=540f6d96a2cd47090028d136566323ab440d235e2397be9060c48a72449d5681\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/1d/15/c87a4302a6c7273ce1b4f282bec3c6877fb2f521535d87d30f\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.2 toposort-1.6\n",
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/70/45d3b9fab768215a2055c7819d39547a4b0b7401b4583094068741aff99b/tensorflow-1.13.2-cp37-cp37m-manylinux1_x86_64.whl (92.7MB)\n",
            "\u001b[K     |████████████████████████████████| 92.7MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.32.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (3.12.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 39.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (56.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.7.4.3)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, keras-applications, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aRoHGh5b_ye",
        "outputId": "1887498b-6e89-4d4a-fb68-41ed745a0276"
      },
      "source": [
        "! pip install lyricsgenius\n",
        "\n",
        "#ignore this, i'm just running on my windows os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lyricsgenius\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/32/be32f6922f70fd1b9900b50b228f6585cd60a96bdf03589df738f627d388/lyricsgenius-3.0.1-py3-none-any.whl (59kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 30kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 40kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from lyricsgenius) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from lyricsgenius) (4.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->lyricsgenius) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->lyricsgenius) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->lyricsgenius) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->lyricsgenius) (2.10)\n",
            "Installing collected packages: lyricsgenius\n",
            "Successfully installed lyricsgenius-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL489TcDdZ9G",
        "outputId": "7c2de9e8-32b9-4b6f-f3be-188a358c3afd"
      },
      "source": [
        "import lyricsgenius as genius \n",
        "\n",
        "file = open(\"./1PLIKÉ140.txt\", \"w\")  #Change the name of the file you want to write the lyrics on it, depends on the artist(s) you want to work on.\n",
        "#You can find your creds by creating an account on : https://genius.com/api-clients\n",
        "creds=\"INSERT YOUR CREDS HERE\"\n",
        "#We are excluding everything that could be a remix, a featuring or a live version, because we want to train our script on the artist.\n",
        "genius = genius.Genius(creds,\n",
        "                             skip_non_songs=True, excluded_terms=[\"(Remix)\", \"(Live)\", \"(Featuring)\", \"(ft.)\", \"featuring\", \"ft.\", \"avec\", \"with\"],\n",
        "                             remove_section_headers=True)\n",
        "\n",
        "artists = ['Freeze Corleone'] # You can choose a lot of artists, here we are focusing on one artist to use GPT on his texts\n",
        "\n",
        "\n",
        "def lyrics_scrapper(arr, k):  # Write lyrics of k songs by each artist in arr\n",
        "    c = 0  \n",
        "    for name in arr:\n",
        "        try:\n",
        "            songs = (genius.search_artist(name, max_songs=k)).songs #We can also change the script to use it without max_songs\n",
        "            s = [song.lyrics for song in songs]\n",
        "            file.write(\"\\n \\n   <End of the song>   \\n \\n\".join(s))  # Allows us and GPT to restrict the number of songs.\n",
        "            c += 1\n",
        "            print(\"Songs found:\"+ len(s))\n",
        "        except:  #\n",
        "            print(\"No song found\")\n",
        "\n",
        "lyrics_scrapper(artists, 50)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Searching for songs by Freeze Corleone...\n",
            "\n",
            "Song 1: \"Freeze Raël\"\n",
            "Song 2: \"Rap catéchisme\"\n",
            "Song 3: \"Intro\"\n",
            "Song 4: \"Hors ligne\"\n",
            "Song 5: \"Desiigner\"\n",
            "Song 6: \"Baton Rouge\"\n",
            "Song 7: \"Sacrifice de Masse\"\n",
            "Song 8: \"Fentanyl\"\n",
            "Song 9: \"Jeremy Lin\"\n",
            "Song 10: \"Welcome to the party (freestyle)\"\n",
            "Song 11: \"Tarkov\"\n",
            "Song 12: \"Chen Laden\"\n",
            "Song 13: \"PDM\"\n",
            "Song 14: \"Madara\"\n",
            "Song 15: \"Scellé Part. 2\"\n",
            "Song 16: \"LRH\"\n",
            "Song 17: \"Donquixote Doflamingo\"\n",
            "Song 18: \"Stretch 4\"\n",
            "Song 19: \"Numérologie\"\n",
            "Song 20: \"S/O Congo Part.2\"\n",
            "Song 21: \"S/o Congo\"\n",
            "Song 22: \"Big Pharma\"\n",
            "Song 23: \"Dans les buissons\"\n",
            "Song 24: \"Ekip\"\n",
            "Song 25: \"3 Planètes\"\n",
            "Song 26: \"Lester\"\n",
            "Song 27: \"Gaucho\"\n",
            "Song 28: \"Logo Audi\"\n",
            "Song 29: \"R.I.P. Pop Smoke\"\n",
            "Song 30: \"Fredo Santana\"\n",
            "Song 31: \"L’art de la guerre\"\n",
            "Song 32: \"Moncler\"\n",
            "Song 33: \"Pas de refrain\"\n",
            "Song 34: \"Mode avion\"\n",
            "Song 35: \"Luffy (freestyle)\"\n",
            "Song 36: \"Q.B\"\n",
            "Song 37: \"TX\"\n",
            "Song 38: \"Recette\"\n",
            "Song 39: \"T.H.C.\"\n",
            "Song 40: \"2016 t’as peur\"\n",
            "Song 41: \"Apu\"\n",
            "Song 42: \"Mage noir\"\n",
            "Song 43: \"Alphonse Karr\"\n",
            "Song 44: \"Benjamins bleus\"\n",
            "Song 45: \"16 pains (Freestyle)\"\n",
            "Song 46: \"Karim et Nico\"\n",
            "Song 47: \"Comme ça\"\n",
            "Song 48: \"Hassan II\"\n",
            "Song 49: \"Lampadaire\"\n",
            "Song 50: \"Requiem for a drill\"\n",
            "\n",
            "Reached user-specified song limit (50).\n",
            "Done. Found 50 songs.\n",
            "No song found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HypqGa4LfkEB"
      },
      "source": [
        "#However, in our lyrics, we have a small issue :\n",
        "In music, there is a real use of syllables, so we want our program to take care of these syllables\n",
        "We have two steps to deal with it :\n",
        "  - First, make a script that cut words into syllable\n",
        "\n",
        "            I woke up this morn–ing and fig–ured I\\'d call you\n",
        "            In case I\\'m not here to|mor|row, I\\'m hop|ing that I can bor|row\n",
        "            A peace of mind, I\\'m be–hind on what\\'s re–al–ly im–por–tant\n",
        "            My mind is re–al–ly dis–tort–ed, I find noth–ing but trou–ble in my life\n",
        "\n",
        "  - (This step is mostly for french music) : We will have to delimit the apostrophes and hyphens by putting spaces before and after.\n",
        "\n",
        "\n",
        "\n",
        "We will focus on this after putting the generator and training him (to avoid wasting time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIITa4u2dgyK",
        "outputId": "31fbe6e9-f0d3-42e6-e893-2aa943cc334d"
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdsWEs5yiGHY",
        "outputId": "7376f3c0-2883-4056-8336-f1b49f75fe55"
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git\n",
        "! cd gpt-2/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 21.25 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImxHTXzeiNXT",
        "outputId": "c9677518-cd77-43c9-9176-dc9740ec3266"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")\n",
        "gpt2.download_gpt2(model_name=\"355M\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 191Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 725kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 239Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:03, 7.82Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 175Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 966kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 987kit/s]\n",
            "Fetching checkpoint: 1.05Mit [00:00, 431Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 581kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 191Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [12:46, 1.85Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 159Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 712kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 969kit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "APd_JmO5k0pp",
        "outputId": "9cd2bc85-19ac-4aaf-a015-cb311de39116"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, dataset='1PLIKÉ140.txt', model_name='124M', steps=200, restore_from='fresh', run_name='rap', print_every=10, sample_every=20, save_every=50)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-547ec3815650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1PLIKÉ140.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'124M'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fresh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rap'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gpt2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "2Hgk6mWTcPuo",
        "outputId": "2b4b0ab6-1da2-43ed-9c30-f324d9c1279a"
      },
      "source": [
        "gpt2.generate(sess, run_name='rap')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9ee503879357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rap'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEVW7WFYcQMT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}